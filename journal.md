# Journal de bord du projet encadr√©
## Travail du 5/10
Aujourd'hui nous avons continu√© avec une camarade le travail fait en classe sur GitHub, cela consist√© en un nouveau fichier dans lequel nous pourrons dans le futur √©crire le travail fait, les difficult√©s rencontr√©es ainsi que les mani√®res de les r√©soudre. Nous avons utilis√© les commandes suivantes :
git status : v√©rifier les modifications
git log : v√©rifier les derniers commits
Nous avons rencontr√© un probl√®me sur le terminal, en effet lorsque nous avons tap√© la commande git log nous avons remarqu√© quelque chose d'√©tonnant : l'auteur du dossier √©tait sous le nom de l'enseignant Yoann Dupont. Nous savons que nous avons clon√© le d√©p√¥t du professeur, et avons pens√© √† garder cette configuration ou cr√©er notre propre d√©p√¥t. Cependant nous nous sommes tout de m√™me pos√© la question suivante : est-ce que le fait que le dossier soit sous son nom nous emp√™che de voir notre propre d√©p√¥t sur le terminal ?
Nous avons √©mis comme hypoth√®se que nous devions clon√© notre propore github et avons d√©cid√© de ne pas le faire et attendre la seance prochaine. 
En ce qui concerne le taf nous avons peur qui ne prenne pas nos modifications en compte mais nous allons tout de m√™me essay√© et nous verrons cela mercredi :) 

# Plusieurs difficult√©s rencontr√©es 
Lors de la mise en place de mon d√©p√¥t pour les exercices, j‚Äôai rencontr√© plusieurs difficult√©s. Au d√©but, j‚Äôeffectuais les exercices et envoyais les tags, mais les enseignants ne pouvaient pas les voir √† cause de probl√®mes li√©s au d√©p√¥t. Cela m‚Äôa oblig√© √† recr√©er un nouveau d√©p√¥t et √† g√©n√©rer un token pour pouvoir envoyer mes modifications correctement.
J‚Äôai demand√© de l‚Äôaide autour de moi pour mieux comprendre le fonctionnement de Git et, apr√®s plusieurs essais, j‚Äôai enfin r√©ussi √† utiliser correctement les commandes git push, git commit, etc. Cela m‚Äôa permis de comprendre le processus et de reprendre confiance dans la gestion de mon d√©p√¥t.
√Ä l‚Äôheure actuelle, je pense que mon d√©p√¥t n‚Äôest pas encore parfaitement organis√©. J‚Äôattends de le montrer aux professeurs pour recevoir leurs conseils, notamment sur le rangement ou l‚Äôorganisation des fichiers. En attendant, je dois refaire mes tags, et je me pose la question de savoir comment d√©placer le premier tag du premier exercice vers le nouveau d√©p√¥t.
Concernant les exercices eux-m√™mes, j‚Äôai pu les terminer, mais j‚Äôai eu quelques difficult√©s √† cause de la consigne assez vague. Il a fallu fouiller dans les diapositives et la documentation pour trouver les commandes n√©cessaires et comprendre comment r√©aliser correctement les exercices.

# Travail du 14/10
## Le commentaire de l'exercice 4: 
Dans ce sript je peux voir des commandes qui permettent de v√©rifier si les lignes d'un fichier ressemblent √† des adresses URL. Cela est permit par le programme qui va lire les lignes du fichier et qui va v√©rifi√© si elle commence par "http://" ou "https://" et ensuite pour chaque ligne il affiche √† l'aide de la commande echo s'il " ressemble √† une URL valide" ou " ne ressemble pas √† une URL valide ". 
A la fin il compte combien √©taient valides (ok) et combien ne l'√©taient ps (nok), puis affiche le total. 

# Mini projet
## exercice 1 question 1: 
il est pr√©ferable d'utiliser while read -r line< fichier pour lire directement le fichier. Si nous utilisons cat, des sous-processus seront cr√©√©s cela peut ralentir les taches et √™tre moins efficace et on conserve bien les variables 

# curl -o /dev/null -s -w 
Au d√©but, je ne comprenais pas vraiment l‚Äôutilit√© de la commande curl -o /dev/null -s -w que le professeur avait utilis√©e en cours. J‚Äôai donc pris le temps de chercher √† comprendre ce que faisaient chacune de ces options. J‚Äôai appris que -o /dev/null permet de ne pas enregistrer le contenu de la page, ce qui est pratique quand on veut juste r√©cup√©rer des informations sans stocker le fichier. L‚Äôoption -s rend la commande silencieuse, en supprimant les messages d‚Äôavancement, et -w permet d‚Äôafficher uniquement certaines informations utiles, comme le code HTTP, l‚Äôencodage ou le temps de r√©ponse. J‚Äôai compris que cette commande est particuli√®rement adapt√©e lorsque l‚Äôon veut analyser plusieurs pages web rapidement, pour v√©rifier si elles sont accessibles ou r√©cup√©rer des m√©tadonn√©es, sans avoir √† t√©l√©charger tout le contenu. Cette recherche m‚Äôa permis de mieux comprendre son fonctionnement et de l‚Äôappliquer ensuite dans mon script pour r√©cup√©rer les codes HTTP et les encodages des pages.
# mini projet 2
Pendant ce miniprojet, j‚Äôai rencontr√© plusieurs probl√®mes li√©s √† l‚Äôutilisation de Git et √† la gestion des fichiers dans mon d√©p√¥t. Tout d‚Äôabord, certains fichiers n‚Äô√©taient pas correctement suivis par Git, ce qui m‚Äôemp√™chait de les ajouter et de les pousser sur le d√©p√¥t distant. J‚Äôai d√ª v√©rifier l‚Äôarborescence des dossiers avec la commande tree et m‚Äôassurer que les chemins vers mes fichiers √©taient corrects avant d‚Äôutiliser git add. Ensuite, j‚Äôai rencontr√© des conflits lors du git pushcar le d√©p√¥t distant avait des commits que je n‚Äôavais pas encore r√©cup√©r√©s. Pour r√©soudre cela, j‚Äôai utilis√© git pull --rebase pour int√©grer les changements distants sans cr√©er de commit de fusion. Enfin, pour les tags, j‚Äôai d√ª faire attention aux tags d√©j√† existants sur le d√©p√¥t distant ; j‚Äôai v√©rifi√© les tags avec git tag et j‚Äôai pouss√© le tag uniquement lorsqu‚Äôil √©tait nouveau, afin d‚Äô√©viter les erreurs de type ‚Äúalready exists‚Äù. Ces √©tapes m‚Äôont permis de mieux comprendre la gestion des fichiers, des commits et des tags sur GitHub et de r√©soudre les erreurs progressivement gr√¢ce √† l‚Äôanalyse des messages d‚Äôerreur et √† la consultation de la documentation Git.

# pour finaliser le mini projet:
Pour finaliser ce mini-projet, j‚Äôai suivi une approche bas√©e sur les exemplaires fournis sur Github. Je consulte les scripts et pages exemples, puis j‚Äôidentifie les √©l√©ments dont j‚Äôai besoin pour mon site (mise en page, tableaux, menu, ic√¥nes, etc.). Ensuite, j‚Äôint√®gre ces √©l√©ments progressivement dans mon propre script, en adaptant le code √† mes besoins et en essayant de conserver la structure et la mise en forme du mod√®le. Cette m√©thode m‚Äôa permis de construire mon site √©tape par √©tape, tout en me basant sur des exemples fonctionnels pour guider mon travail.
## petit probl√®me avec le site
Lors de la finalisation du site, j‚Äôai rencontr√© un probl√®me avec l‚Äôaffichage de l‚Äôimage sur GitHub Pages. M√™me apr√®s avoir v√©rifi√© et corrig√© le chemin du dossier images dans le terminal et dans le code HTML, l‚Äôimage ne s‚Äôaffiche toujours pas sur le site. J‚Äôai essay√© plusieurs m√©thodes pour r√©soudre le probl√®me, mais je n‚Äôai pas r√©ussi √† trouver une solution avant le dernier d√©lai de rendu. Je vais tout de m√™me envoyer le site tel quel pour ce rendu, mais je continuerai √† chercher la solution afin de corriger l‚Äôaffichage de l‚Äôimage et mettre √† jour le site apr√®s coup. 
Cependant je pense (j'ai l'impressionü•≤) le reste du site fonctionne normalement : la mise en page avec Bulma, les tableaux fran√ßais et anglais, le menu lat√©ral et les d√©monstrations d‚Äôic√¥nes FontAwesome sont op√©rationnels.
Cela m‚Äôa permis de bien comprendre l‚Äôimportance des chemins exacts, de la sensibilit√© aux majuscules/minuscules et des particularit√©s de GitHub Pages.
### update 
J‚Äôai enfin trouv√© la solution du probl√®me par rapport √† l‚Äôimage : pour le r√©soudre, j‚Äôai identifi√© que le chemin entre l‚Äôindex.html et le dossier images √©tait incorrect, car le fichier index √©tait lui-m√™me dans le sous-dossier tableaux, alors que le dossier images se trouvait √† c√¥t√©. Ce que j‚Äôai fait : j‚Äôai tout simplement d√©plac√© index.html directement dans le dossier miniprojet, ce qui a permis √† l‚Äôimage d‚Äô√™tre correctement trouv√©e et affich√©e. J‚Äôai ensuite mis √† jour mon d√©p√¥t Git avec git add, git commit et git push. Apr√®s ces √©tapes, l‚Äôimage s‚Äôaffiche correctement sur le site, et le probl√®me est d√©sormais r√©solu.
Apr√®s avoir r√©solu le probl√®me de l‚Äôimage, j‚Äôai rencontr√© un nouveau probl√®me‚ÄØ: le site s‚Äôaffiche parfaitement lorsque je l‚Äôouvre directement sur mon Mac, mais je me suis rendu compte en cours qu‚Äôil ne s‚Äôaffichait pas correctement lorsqu‚Äôil √©tait publi√© sur GitHub Pages. Il s‚Äôagit donc d‚Äôun autre probl√®me avec la fa√ßon dont les fichiers sont mis en ligne ou avec les chemins des fichiers.  
## d√©couverte du projet final: 
# je fais le point sur ce que je sais faire et ce qui m‚Äôattend 
Ce que je ma√Ætrise d√©j√† (Merci le Mini-Projet !)
Le mini-projet m'a permis de valider les briques de base. Je n'ai plus peur de :


* ‚Ä®La boucle principale : Lire un fichier d'URLs ligne par ligne, √ßa, c'est acquis.‚Ä®‚Ä®
* ‚Ä®Les arguments : Passer un fichier √† mon script ($1) et v√©rifier qu'il est bien l√†, c'est bon.‚Ä®‚Ä®
* ‚Ä®R√©cup√©rer l'info : J'ai d√©j√† le code pour choper le code HTTP et l'encodage.‚Ä®‚Ä®
En gros, toute la structure "squelette" de mon script est d√©j√† √©crite. Je ne pars pas de z√©ro page blanche, et √ßa, √ßa rassure √©norm√©ment.
Ce qui change (et o√π je dois faire attention)
Par contre, en lisant le PDF du projet final, je vois que le niveau monte d'un cran. Ce n'est plus juste de l'affichage, c'est de la production.
1. Fini le terminal, bonjour les fichiers Dans le mini-projet, on faisait des echo dans le terminal. L√†, c'est du s√©rieux : je dois stocker physiquement les pages. Je vais devoir g√©rer des redirections > vers des dossiers pr√©cis (aspirations, dumps-text). Si je me rate dans les chemins de dossiers, rien ne marchera.‚Ä®‚Ä®‚Ä®
2. L'encodage : C'est la grosse diff√©rence. Avant, on d√©tectait l'encodage et c'est tout. L√†, le script doit √™tre intelligent :
    * Si c'est UTF-8 : super, on continue.
    * Si ce n'est PAS UTF-8 : je ne peux plus ignorer le probl√®me. Je dois utiliser iconv pour convertir la page. C'est crucial pour que egrep fonctionne apr√®s. C'est la partie conditionnelle (if/else) qui va √™tre la plus d√©licate √† coder.‚Ä®‚Ä®‚Ä®
3. Le r√©sultat : Une vraie page Web Je ne dois plus sortir un tableau texte moche, mais une vraie page HTML. √áa veut dire que mon script doit "√©crire du HTML" (des balises <tr>, <td>, <a href=...>).‚Ä®‚Ä®
4. L'analyse du contenu Le mini-projet s'arr√™tait √† "r√©cup√©rer la page". L√†, je dois creuser dedans :
    * Utiliser lynx -dump pour virer le HTML et garder le texte pur.‚Ä®‚Ä®
    * Utiliser egrep pour attraper le contexte autour de mon mot-cl√©.‚Ä®‚Ä®
# mettre au clair la structure du script 
La structure de mon script devra ressembler √† √ßa :
1. ‚Ä®Lecture des URLs : Je pars d'un fichier texte contenant mes 50 URLs.‚Ä®‚Ä®
2. ‚Ä®La Boucle : Je dois faire une boucle for pour traiter chaque URL une par une.‚Ä®‚Ä®
3. V√©rification de l'encodage : C'est le point critique. Si la page n'est pas en UTF-8, je dois essayer de d√©tecter l'encodage et le convertir (avec iconv). Sinon, j'aurai des probl√®mes d'affichage des caract√®res.‚Ä®‚Ä®‚Ä®
4. ‚Ä®Extraction du contexte : Une fois que j'ai le texte propre (gr√¢ce √† lynx), je dois utiliser egrep pour trouver mon mot-cl√© et afficher les lignes autour (le contexte).‚Ä®‚Ä®
Le rendu final : Je ne dois pas oublier que tout cela doit atterrir dans un Tableau HTML bien structur√©. Pour chaque URL, je dois g√©n√©rer une ligne de tableau avec :


* Le lien vers la page originale.
* Le code HTTP.
* L'encodage d√©tect√©.
* Le lien vers mon fichier "dump" (texte brut).
* Le contexte (la concordance) 
* 
# je revois le cours d‚Äôhtml et http ‚Ä®
Le cours rappelle que le HTML n'est finalement que du texte balis√©. C'est important pour moi car mon but ultime dans ce projet est de nettoyer tout ce balisage (le head, les styles, etc.) pour ne garder que le vrai texte.



Mais avant d'avoir la page, il y a le protocole HTTP. J'ai retenu le sch√©ma requ√™te/r√©ponse. C'est super important pour mon script : quand je demande une URL, je dois v√©rifier le Code HTTP de la r√©ponse.




* Si c'est 200, c'est gagn√©, je peux traiter la page.‚Ä®‚Ä®
* Si c'est 4xx ou 5xx (erreur client ou serveur), mon script doit √™tre assez intelligent pour ignorer cette URL et passer √† la suivante sans planter.‚Ä®‚Ä®
2. Ma bo√Æte √† outils : cURL, Wget et Lynx
C'est l√† que je vois mes progr√®s. Au d√©but, je ne comprenais pas la diff√©rence entre ces commandes. Maintenant, c'est plus clair pour le projet :
* ‚Ä®Pour r√©cup√©rer la page (l'aspiration) : Je vais utiliser cURL ou wget. cURL est pratique pour voir les ent√™tes (avec l'option -i) et v√©rifier l'encodage avant m√™me de t√©l√©charger.‚Ä®‚Ä®‚Ä®‚Ä®
* Pour nettoyer le texte : C'est l√† que Lynx intervient. C'est un navigateur textuel, mais je vais surtout l'utiliser en ligne de commande avec l'option -dump. √áa me permet de r√©cup√©rer le contenu textuel brut sans navigation, ce qui est exactement ce qu'on attend dans la colonne "dump" du tableau final.‚Ä®‚Ä®
